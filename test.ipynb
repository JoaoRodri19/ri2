{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path):\n",
    "  data = []\n",
    "  with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "      json_data = json.loads(line)\n",
    "      data.append(json_data)\n",
    "  return data\n",
    "\n",
    "def load_json(file_path):\n",
    "  with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "  return data\n",
    "\n",
    "def load_partial_json(file_path, num_lines=10):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for _ in range(num_lines):\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            try:\n",
    "                json_data = json.loads(line)\n",
    "                data.append(json_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection of texts\n",
    "\n",
    "if not os.path.exists(\"datasets/collection_with_texts.jsonl\"):\n",
    "\n",
    "        collection_with_texts = list()\n",
    "\n",
    "        with gzip.open(\"datasets/pubmed_2022_tiny.jsonl.gz\", 'rb') as gz_file:\n",
    "                for line in gz_file:\n",
    "                        res = line.decode('utf-8')\n",
    "                        res = json.loads(res)\n",
    "                        collection_with_texts.append({\"id\": res[\"pmid\"], \"text\" : res[\"title\"] + \" \" + res[\"abstract\"]})\n",
    "\n",
    "\n",
    "        collection_with_texts_id = list()\n",
    "\n",
    "        for id in collection_with_texts:\n",
    "                collection_with_texts_id.append(id[\"id\"])\n",
    "\n",
    "        positive = load_jsonl(\"datasets/RI_2023_training_data_wContents.jsonl\")\n",
    "\n",
    "        positive_text = list()\n",
    "        list_of_ids = list()\n",
    "\n",
    "        for pos in positive:\n",
    "                for id in pos[\"documents\"]:\n",
    "                        if id not in list_of_ids:\n",
    "                                list_of_ids.append(id)\n",
    "                                positive_text.append({\"id\": id[\"id\"], \"text\": id[\"text\"]})\n",
    "\n",
    "        for i in positive_text:\n",
    "                if i[\"id\"] not in collection_with_texts_id:\n",
    "                        collection_with_texts.append(i)\n",
    "\n",
    "        with open(\"datasets/collection_with_texts.jsonl\", \"w\") as jsonl_file:\n",
    "\n",
    "                for entry in collection_with_texts:\n",
    "                # Convert each dictionary to a JSON string and write it as a line\n",
    "                        json_line = json.dumps(entry)\n",
    "                        jsonl_file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_positive_docs\n",
    "\n",
    "dataset_positive = list()\n",
    "\n",
    "positive = load_jsonl(\"datasets/RI_2023_training_data_wContents.jsonl\")\n",
    "for p in positive:\n",
    "        docs_list = list()\n",
    "        for d in p[\"documents\"]:\n",
    "                docs_list.append(d[\"id\"])\n",
    "        dataset_positive.append({\"body\": p[\"body\"], \"documents\": docs_list, \"id\": p[\"id\"]})\n",
    "\n",
    "with open(\"datasets/train_dataset_positive.jsonl\", \"w\") as jsonl_file:\n",
    "\n",
    "        for entry in dataset_positive:\n",
    "        # Convert each dictionary to a JSON string and write it as a line\n",
    "                json_line = json.dumps(entry)\n",
    "                jsonl_file.write(json_line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "negative = load_jsonl(\"datasets/train_dataset_negative.jsonl\")\n",
    "positive = load_jsonl(\"datasets/train_dataset_positive.jsonl\")\n",
    "i = 0\n",
    "k = 0\n",
    "while i<=len(negative):\n",
    "    try:\n",
    "        neg = negative[i]\n",
    "        pos = positive[k]\n",
    "    except:\n",
    "        break\n",
    "    if pos[\"documents\"] in neg[\"neg_docs\"]:\n",
    "        pos_set = set(pos[\"documents\"])\n",
    "        neg_set = set(neg[\"neg_docs\"])\n",
    "        common_docs = pos_set.intersection(neg_set)\n",
    "        neg_set -= common_docs\n",
    "        neg[\"neg_docs\"] = list(neg_set)\n",
    "    \n",
    "    if len(neg[\"neg_docs\"]) <= len(pos[\"documents\"]):\n",
    "\n",
    "        positive.pop(k)\n",
    "        negative.pop(k)\n",
    "        k= k-1\n",
    "        i = i-1\n",
    "\n",
    "    k=k+1\n",
    "    i=i+1\n",
    "   \n",
    "\n",
    "with jsonlines.open('datasets/positive_filtered.jsonl', 'w') as writer:\n",
    "    writer.write_all(positive)\n",
    "with jsonlines.open('datasets/negative_filtered.jsonl', 'w') as writer:\n",
    "    writer.write_all(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_qrels, InferenceRankingIterator,  BioASQPointwiseIterator, InferenceDataset, create_training_dataset\n",
    "from sampler import BasicSampler\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# torch.utils.data.Dataset (Iterable)\n",
    "train_ds = create_training_dataset(\"datasets/positive_filtered.jsonl\", # \"body\"/\"question\" q_id:[doc_id] 60K\n",
    "                                   \"datasets/negative_filtered.jsonl\", # q_id:[{\"id\":doc_id, \"score\": doc_id}]\n",
    "                                   \"datasets/collection_with_texts.jsonl\", # doc_id: text (title + \" \" + abstract)\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    iterator_class=BioASQPointwiseIterator[BasicSampler],\n",
    "                                    #max_questions=500, # debug\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = load_jsonl(\"datasets/question_E8B1_gs.jsonl\")\n",
    "\n",
    "key_mapping = {\"query_id\": \"id\", \"query_text\": \"body\", \"documents_pmid\": \"documents\"}\n",
    "\n",
    "dev_gs = list()\n",
    "\n",
    "for d in gs:\n",
    "\n",
    "        dev_gs.append({key_mapping[old_key]: value for old_key, value in d.items()})\n",
    "\n",
    "with open(\"datasets/dev_gs.jsonl\", \"w\") as jsonl_file:\n",
    "\n",
    "        for entry in dev_gs:\n",
    "        # Convert each dictionary to a JSON string and write it as a line\n",
    "                json_line = json.dumps(entry)\n",
    "                jsonl_file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_bm25 = load_json(\"datasets/scores_bm25.json\")\n",
    "queries = load_jsonl(\"datasets/question_E8B1_gs.jsonl\")\n",
    "\n",
    "queries_text = dict()\n",
    "\n",
    "for query in queries:\n",
    "    queries_text[query[\"query_id\"]] = query[\"query_text\"]\n",
    "\n",
    "convert = list()\n",
    "for score in scores_bm25:\n",
    "    \n",
    "    docs = list()\n",
    "    for doc in scores_bm25[score]:\n",
    "        docs.append({\"id\": doc, \"score\":50}) ## nÃ£o gerei os dados com o score\n",
    "    convert.append({\"id\": score, \"documents\": docs, \"question\":queries_text[score]})\n",
    "\n",
    "with open(\"datasets/scores_bm25.jsonl\", 'w') as jsonl_file:\n",
    "    for entry in convert:\n",
    "        json_line = json.dumps(entry)\n",
    "        jsonl_file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = load_jsonl(\"datasets/scores_bm25.jsonl\")\n",
    "golden = load_jsonl(\"datasets/dev_gs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_ids = list()\n",
    "\n",
    "for i in bm25:\n",
    "        bm25_ids.append(i[\"id\"])\n",
    "\n",
    "golden_ids = list()\n",
    "\n",
    "for i in golden:\n",
    "        golden_ids.append(i[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.utils.data.Dataset (Iterable)\n",
    "dev_ds = InferenceDataset(\"datasets/scores_bm25.jsonl\", #q_id:[doc_id ->>] BM25 top-1000 top-100 # 100 question -> 1000 docs\n",
    "                          train_ds.collection,\n",
    "                          tokenizer,\n",
    "                          #max_questions=10, # debug\n",
    "                          at=100, #max docs\n",
    "                          gs_path=\"datasets/dev_gs.jsonl\", # q_id: [doc_id]\n",
    "                          iterator_class=InferenceRankingIterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "id2label = {0: \"IRRELEVANT\", 1: \"RELEVANT\"}\n",
    "label2id = {\"IRRELEVANT\": 0, \"RELEVANT\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")#.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data.BioASQDataset"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2003, 15578, 4588, 14854, 2594, 5648, 2109, 2005, 3949, 1997, 3078, 12170, 6632, 2854, 16480, 25023, 13706, 1029, 102, 15578, 4588, 14854, 2594, 5648, 2005, 1996, 3949, 1997, 3078, 12170, 6632, 2854, 25022, 12171, 25229, 1012, 4955, 2045, 2003, 3278, 4895, 11368, 2342, 1999, 3078, 12170, 6632, 2854, 16480, 25023, 13706, 1006, 1052, 9818, 1007, 1999, 5022, 2104, 1011, 26651, 2000, 1996, 2069, 4844, 7242, 24471, 6499, 3207, 11636, 17994, 23518, 5648, 1006, 20904, 3540, 1007, 2040, 2024, 2012, 3445, 3891, 1997, 27673, 2000, 2203, 1011, 2754, 11290, 4295, 1012, 15578, 4588, 14854, 2594, 5648, 1006, 1051, 3540, 1007, 2003, 1037, 2521, 5267, 9314, 1060, 10769, 1006, 23292, 2099, 1007, 3283, 26942, 2029, 2038, 2042, 16330, 2004, 1037, 2117, 2240, 7242, 1999, 1052, 9818, 1998, 2038, 3728, 2042, 11172, 2094, 2011, 1996, 17473, 1012, 2752, 3139, 1996, 6887, 27292, 22684, 6483, 1998, 7366, 1997, 1051, 3540, 2004, 2019, 23292, 2099, 3283, 26942, 1998, 2049, 6612, 6666, 1012, 1037, 11778, 3319, 2001, 10607, 1997, 2405, 3906, 1010, 3116, 29474, 1998, 3979, 20588, 21011, 2478, 1996, 3945, 3408, 23292, 2099, 1010, 1042, 25708, 1011, 2539, 1006, 1004, 1042, 25708, 1011, 2321, 1007, 1010, 15578, 4588, 14854, 2594, 5648, 1998, 20014, 1011, 25374, 1012, 6739, 8570, 1024, 1051, 3540, 13416, 7524, 2000, 11704, 18479, 20200, 23974, 12737, 2083, 7312, 1999, 23974, 5648, 10752, 1006, 2011, 3622, 1998, 14958, 1006, 3081, 4607, 10085, 17250, 1011, 2207, 1042, 25708, 16147, 1007, 4506, 2006, 22330, 2361, 2581, 27717, 1011, 19872, 23974, 5648, 10752, 1007, 1998, 23974, 5648, 4654, 16748, 3508, 2011, 2002, 4502, 3406, 27321, 1012, 2009, 6022, 24840, 11290, 16012, 15869, 11709, 6118, 3378, 2007, 3891, 1997, 4295, 14967, 1999, 20904, 3540, 2104, 1011, 26651, 5022, 1998, 1996, 3145, 2217, 1011, 3466, 1997, 10975, 9496, 5809, 2064, 2022, 4359, 2011, 23569, 27605, 6924, 9998, 2075, 1012, 1051, 3540, 2097, 2022, 1996, 2034, 2358, 8609, 7810, 7242, 3107, 1999, 1052, 9818, 1010, 2174, 12210, 14049, 3979, 1998, 2613, 2166, 2951, 2024, 2734, 2000, 12210, 2008, 6592, 3512, 16012, 15869, 8377, 2024, 10349, 2011, 7620, 1999, 3145, 6612, 13105, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label_ids': 1}\n",
      "{'input_ids': [101, 1037, 6805, 2013, 1996, 10459, 2732, 16356, 2572, 6321, 5358, 2863, 2137, 2819, 1010, 2064, 3426, 1996, 6778, 2000, 2468, 27395, 2000, 2417, 6240, 1010, 2748, 2030, 2053, 1029, 102, 10788, 1997, 6174, 6582, 8464, 2572, 6321, 5358, 18900, 2483, 1998, 15501, 12190, 11319, 2050, 15775, 16020, 9911, 1999, 2572, 6321, 5358, 2863, 2137, 2819, 21490, 2075, 2048, 3923, 6328, 1999, 5858, 1012, 2005, 1996, 2627, 2382, 2086, 1010, 1996, 2193, 1997, 2111, 10372, 2007, 6187, 10383, 6024, 6074, 1997, 15501, 12190, 11319, 12650, 1010, 6857, 3137, 7282, 9016, 1010, 1998, 7282, 9016, 2177, 6174, 18319, 12650, 1006, 16420, 16523, 1007, 2038, 3445, 1999, 5858, 1012, 2174, 1010, 2045, 2003, 1037, 3768, 1997, 2951, 2006, 26835, 20272, 2306, 3923, 10058, 1012, 2000, 14358, 1996, 20272, 1997, 16356, 1011, 15356, 26835, 2015, 1999, 2367, 10058, 1010, 4724, 2549, 2572, 6321, 5358, 2863, 2137, 2819, 1006, 10459, 2732, 1007, 16356, 2015, 2020, 5067, 2013, 1996, 4044, 1999, 2048, 6328, 1999, 21773, 1010, 5858, 1012, 1996, 3739, 1997, 15501, 12190, 11319, 2050, 26924, 1012, 1998, 7282, 9016, 2177, 1006, 16420, 2290, 1007, 6174, 6582, 8464, 26924, 1012, 2001, 4340, 2478, 20155, 2613, 1011, 2051, 17782, 11022, 4677, 4668, 1006, 1053, 15042, 2099, 1007, 1012, 3943, 1012, 1020, 1003, 1006, 16333, 1013, 4724, 2549, 1007, 1997, 1996, 1037, 1012, 2137, 2819, 16356, 2015, 2020, 3893, 2005, 6174, 6582, 8464, 2572, 6321, 5358, 18900, 2483, 1998, 2321, 1012, 1016, 1003, 1006, 5764, 1013, 4724, 2549, 1007, 2020, 3893, 2005, 15501, 12190, 11319, 2050, 15775, 16020, 9911, 1012, 2053, 16356, 2015, 2020, 3893, 2005, 2060, 16420, 2290, 6174, 6582, 8464, 2063, 1006, 1054, 1012, 6174, 6582, 5332, 2072, 1010, 1054, 1012, 6262, 2072, 1007, 2030, 2060, 15501, 12190, 11319, 6679, 1006, 1041, 1012, 24023, 6137, 1010, 1998, 6090, 6030, 3137, 15501, 12190, 11319, 2050, 1007, 1012, 2122, 2913, 3073, 3445, 4824, 1997, 1996, 4022, 3891, 2005, 8087, 2075, 16356, 1011, 15356, 26835, 2015, 1999, 3923, 10058, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label_ids': 0}\n"
     ]
    }
   ],
   "source": [
    "_iter = iter(train_ds)\n",
    "\n",
    "print(next(_iter))\n",
    "print(next(_iter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FCD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
